# 对于(周期的？)时间序列预测任务，可以使用滑动窗口法来融合不同的特征，构造新的数据****(https://baijiahao.baidu.com/s?id=1610773249505904730&wfr=spider&for=pc)
“训练集构建方面有一些值得注意的地方。直接使用submit中的skuid来构建训练集并不是最合适的选择，可以考虑自己根据官方选择submit中skuid的方式自己在线下选择一些skuid来进行训练集的构建。”
“目前没有发现有效的强特，也就是俗称的trick。暂时都是一些简单的特征，各种key值的统计值，例如最大值、最小值、均值、方差、峰度、偏度等等，几十维都有比较好的结果。也可以做一些转化率的特征，对于体现商品的热度也是很有效果的。由于是回归问题，特征值差异过大会让结果拟合不到一个比较好的效果，可以考虑特征归一化。”（http://wemedia.ifeng.com/82099572/wemedia.shtml）
# 在时间序列预测任务中，数据往往可能存在不同寻常的大幅波动，例如商家双11活动、法定节假日、数据缺失等导致的数据过多或过少，对于这种情况可以采用平滑来处理，或者直接删去该部分数据
# 时间序列预测任务，不完全等同于机器学习任务，可以考虑使用RNN来建模：https://github.com/drop-out/RNN-Active-User-Forecast

# 指数平滑预测法是一种确定性的平滑预测法。其实质是：通过计算指数平滑平均数来平滑时间序列，消除历史统计序列中的随机波动，以找出其主要发展趋势。根据设置参数的不同可以分为单指数预测、双指数预测和三指数预测。其中，单指数具有一个参数，适合于具有平稳性特征时间序列的预测，也称为平稳性预测。双指数预测具有两个参数，适合于具有线性趋势性特征时间序列的预测，也称为趋势性预测。三指数预测具有三个参数，适合于具有趋势和季节性或周期性特征时间序列的预测，也称为季节性或周期性预测。在移动平均方法中，对每个数据赋予相同的权重，而指数平滑可以根据参数对数据赋予不同的权重，这样就可以获得更好的拟合曲线和预测结果。（https://blog.csdn.net/SUSU0203/article/details/79981644）

# “以及一些数据平滑，比如同一天点击数超过15就平滑为15，超过10次的购买平滑为10次。当然还有比较专业的平滑算法，如移动平滑，指数平滑。”数据平滑处理一般是用来减少数据的统计误差的，修复异常数据；还有移动平均滤波、加权移动平均滤波

# 广告CTR的贝叶斯平滑：https://blog.csdn.net/jinping_shi/article/details/78334362
                     https://blog.csdn.net/mytestmy/article/details/19088519
                     https://blog.csdn.net/dengxing1234/article/details/77965536


#########################################################################################

# 数据清洗：
  处理缺失值(用-999填充等)、从训练集其他特征中预测出可能的缺失值
  剔除异常离群点：规定一个置信水平，确定一个置信区间，超过该区间的即认为是异常点
  清洗文本特征中的无意义字符或乱码，如@，%，表情等
  处理特征值不一致的情况，如“BEIJING”与“beijing”应视为同一特征值，也可以使用这种方式来归并特征值，减少维度
  数值特征的归一化、标准化、分箱等变换
  “对于数值变量，我们通常会先进行归一化处理，这样有利于我们加快收敛速度，将各个维度限制在差不多的区间内，对一些基于距离的分类器有着非常大的好处，但是对于决策树一类的算法其实就没有意义了”
  “数据和特征决定了机器学习的上限，而模型和方法只能是逼近这个上限”
  
  “有个问题就是正负样本不平衡，常规的方法是重采用正样本，欠采样负样本。Bootstrap 采样是常用的重采样方法，简单的说就是有放回的抽样正样本。”



# EDA：
  舍去无用的特征（列）：
      缺失值占99%的特征；
      某一特征值占所有特征值99%以上的特征；
      两个完全对应或互补(相关性过高，99%以上)的特征，可以去掉一个；与标签相关性太低的特征可以去掉？？————“Tested correlations between columns, picked up pairs whose corr is greater than 0.99, compared the distribution of the features in the pairs and corr with the label, and selected the minor column for elimination.”
      
  *把特征分为数值特征、分类特征来分别处理，（注意特殊形式的特征，如多值离散特征、向量特征），观察特征在标签上的分布（通过可视化），找出相关性高的特征
  使用lgb查看模型重要度
  
  “在处理categorical feature有两点值得注意： 
      如果特征中包含大量需要做dummy variable处理的，那么很可能导致得到一个稀疏的dataframe，这时候最好用下PCA做降维处理。 
      如果某个特征有好几万个取值，那么用dummy variable就并不现实了，这时候可以用Count-Based Learning. 
    （更新）近期在kaggle成功的案例中发现，对于类别特征，在模型中加入tf-idf总是有效果的。 
      还有个方法叫“Leave-one-out” encoding，也可以处理类别特征种类过多的问题，实测效果不错。 ”

  “仔细观察一个模型的部分依赖图 (Partial Dependency Plot，PDP) ， 可以帮助大家理解output是如何随着特征而变化的。问题是，这种图像是用训练好的模型做   的。而如果直接用训练数据来做图，就可以让大家更好地了解underlying data。”



# 特征工程：在原始数据集上，手动挖掘、构建新特征
  统计特征：统计某个特征的count、sum、sum/count等leak特征
  
  “组合统计特征，就是把各种属性类别类特征进行组合，然后统计它们在某一时间段内的行为。举个例子，比如user_id和item_id这两个特征，分别代表用户名和商品名，但是同一个用户可能看过好几个商品(反过来也成立，即一个商品可能会被多个用户看过)，所以可以统计该用户在同一天内看过的商品个数，以及在这一天内，该用户在看过了某个商品之前已经看过了多少个商品，或者在看某个商品之后还会看多少个商品，这就对这个用户的行为进行了一定的统计。当然，统计了同一天内，还可以再统计一个小时内，一分钟内，甚至是一秒钟内用户看商品的行为。一秒钟内一个用户看好几个商品的情况是有可能出现的，原因可能是这个用户同一秒钟疯狂地点击好多个广告，或者是因为网络原因这个用户总是点击同一个广告，这些都是有可能的。另外，除了统计总数，还可以统计这个用户看到的商品有多少是唯一的，例如用户一天内看了代号为1,2,2,3,3这一共5个商品，按照上文叙述，这里会统计为5，但是唯一的商品只有1,2,3这三种，所以唯一商品的总数为3，这也可以作为一个特征，再者，二者的比例同样也可以作为一个特征，即3/5。同时，这里也可以整理出商品的一些特征，比如代号为3的商品出现了两次，那么这个商品就可以构造出2/5这样的特征，即某个用户在某个时间段内，他看过的某个商品占他所有看过的商品的比例。诸如此类的组合统计特征可以构造出相当多数量，关键点就在于怎样才能构造出有利于预测转化率的特征组合，然后对其进行行为统计。”
  “时间差特征，这种特征构造开始也需要像组合统计特征一样，要先对需要进行时间差计算的特征进行组合。还是以user_id和item_id特征为例，一个用户看完当前商品之后，他可能还会再看另外一个商品，即点击另外一个广告，那么统计这二者的时间差或许会有所帮助。时间差有两类，一类是当前点击距离下次点击的时间，另一类是当前点击距离上一次点击的时间。”
  “转化率特征，就是利用某种特征的历史转化率来构造特征。举个例子，某个item_id，发现它在过去几天中被人们点击数是10，购买数是8，那么这个商品就有8/10的转化率，而另外一个item_id点击和购买数为10和2，那么转化率就是2/10，大大低于前者，所以就可以给前者设定一个比较大的数值而给后者设定一个比较小的数值，这便构造出了转化率特征。那么倘若你要预测的这天有的商品没在前几天出现怎么办，不是没有转化率特征了么？设为0认为其历史转化率为0？这显然不符合实际，这时候就需要贝叶斯平滑算法了，来利用先验给其设定一个初值。而贝叶斯平滑的具体原理可能需要单独学习一下。这里就不说了。需要注意的是转化率特征一些事项，在我的个人实验中，是这个类别属性的多样性越大越好，比如用户的性别id，最多就三种情况，男或女或用户没有设置，对这三种情况进行贝叶斯平滑效果不见得有多好，而item_id，会有成千上万种，这类属性进行转化率特征构造可能会好得多。另外，千万不要构造当天的转化率特征来预测当天的情况，比如我7号的训练集，我构造了7号的item_id的转化率，然后再拿7号训练集的一部分作为训练集，一部分作为验证集，当然验证结果会表现的很好，但是要是用在预测你的预测集数据上，提交结果之后会发现线上效果很差，因为，已经过拟合了。这里我用了除最后一天的数据来计算各种属性的转化率，这也是复赛中唯一一次用了除最后一天的训练数据。”
  “其中有三个字符串特征，分别是item_category_list,item_property_list和predict_category_property，意思是商品的类别，商品的属性和商品的预测类别和属性。这三个特征是字符串型的，无法直接输入到模型里，但直接扔了不用有点可惜。我在这里没进行多少处理，就是把预测类别属性的字符串给拆开，分成预测类别和预测属性，然后对比商品的真实类别和属性，计算它的预测命中率，即它的预测属性有多少在真实属性中出现过，再除以它的预测属性总数便得到了命中率。这个命中率我想也能反映出一些问题吧，毕竟如果能够精确地猜出用户输入的搜索关键词对应的类别和属性，那么用户的使用体验也会提高，购买商品的欲望也会强一些。我之前在初赛的时候有参考过天池官方论坛上的这篇文章,作者提出可以用文本分类中的TF-IDF的方法去处理predict_category_property这个属性，即把这个属性经TF-IDF处理后变成一个稀疏矩阵，再把这个矩阵输入到一个模型里预测结果，再将结果作为一个特征。我尝试了一下发现对预测结果影响不大就没使用了，或许是自己的操作不是很正确，就没有再去管它了，但是这个想法我觉得很好。”
  
  “另外一些组合特征，比如用户当次搜索距离当天第一次搜索该商品、店铺、类目的时间差，用户当次搜索距离当天最后一次搜索该商品、店铺、类目的时间差；一些商品的相对特征，商品相对同类别、品牌的平均价格差，销量差”
  “组合特征：用户当日、当小时搜索总数；用户当日搜索该商品、店铺、类目次；数用户当小时搜索该商品、店铺、类目次数”
  “统计特征：用户、商品、店铺、品牌、类目在这天之前的买入总数；用户、商品、店铺、品牌、类目前1、2、3、4天之前的转化率；商品的属性是否出现在预测属性中（商品属性是否符合用户要求）；商品属性中有多少个出现在预测属性中；商品属性对的转化率；用户、商品、店铺、品牌前30分钟、1、4、12个小时的浏览量，用户前4个小时的浏览量处于前24个小时的浏览量(相当于对浏览量做了归一化处理) ”
  “ 对用户、商品、店铺、品牌、类别这5列的CVR拿过来做一个融合，融合成一列，因为这些CVR存在很多-1的数据，不利于模型的训练。融合后希望尽量避免-1的数据同时又比较接机原始数据。表现出的意思就是，只有当5列同时为-1，融合数据才为-1，否则不是。”
  “复赛的数据是一样的，但是有这两个不同的地方。第一，复赛的数据多达1000多万，对于特征的提取带来一些问题。第二，初赛的转化率差不多每天都一样，但是复赛不同需要预测特殊日期下的转化率，前7天数据是正常日期下的数据，而第8天开始搞促销，训练数据里面包含第7天上午的数据，我们需要预测第八天下午用户的转化率。
  对于第一个问题，我们对于特征的提取采用多线程的办法，原始数据的大小是10多个G，内存小于16G的电脑可能读不进来。读进来后把数据分成64份，分别保存起来。在后面做特征的时候，创建64个进程，处理各自的数据，如果是和时间相关的数据(需要用到其他时间段的数据)，则需要把数据全部读出来做特征。
  对于第二个问题，一开始的想法是只对第八天上午做训练，然后预测第八天下午数据，结果发现效果一般，同时丢失了前7天很多有用的信息。之后考虑了另一种方案，前7天只提取特征，然后把拿来的特征放到第八天做训练。比如提取的特征有：用户、店铺、商品、品牌、类别在前七天的购买量、浏览量、转化率，还可以有交叉特征，用户前七天浏览所有商品、店铺、类别、品牌的数量，店铺在前七天卖出商品、品牌、类别的数量等等。提交后发现这个效果还可以。
  对模型的优化，只训练第七天，数据量也有限，为了更好的利用数据，可以使用全部的数据做训练，以当前天之前的数据作为特征提取的区间，这样的话，第一天是获取不到历史数据的，我们考虑将第1天的数据删除。”
  “特征。时间上的划分根据转化率来，可以看出晚上转化率略高(大家下班回家了)。店铺评分都差不多，但是分箱之后与转化率的相关性增强，同时利于做交叉特征。通过点击的特征提取出用户喜好。排序特征，比如销量、价格的排序，然后做归一化处理，排名/总数。Item的属性不要那么多，只列出常见的100多种属性拿来提取特征。统计每个人在属性上的平均值，类似于用户的属性喜好。统计item在这些属性上的点击率(商品的欢迎程度)。”
  
  
  “另外还有一种对我启发比较大的feature是对description出现频率最高的15k单词进行一个one-hot深度xgboost训练，将这个训练出来模型的预测结果作为description的encoding。”
  
  数值特征的加减乘除、平方、运算组合：
  
  对原始特征进行切片，提取出重要的信息作为单独一个特征
  
  偏好特征：

  “从目标到特征的数据泄漏导致过度拟合。 泄露的特征具有很高的功能重要性。 但是，要理解为什么在特征中会发生泄漏，这很是困难的”


# 特征选择：
  特征是不是越多越好呢，这可不一定。特征数量较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，容易导致如下的后果：特征个数越多，分析特征、   训练模型所需的时间就越长。容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。
  特征选择的意思就是通过算法能够得到前n个特征组合，使得分类器的error rate 最小，即这样组合最具有判别力。
  一般特征选择是利用相关系数，好的特征子集所包含的特征应该是与分类的相关度较高（相关度高），而特征之间相关度较低的（亢余度低）。可以使用线性相关系数   (correlation coefficient) 来衡量向量之间线性相关度。
  # 主成分分析PCA、公共空间模式CSP





# 模拟训练与集成：
  Python包：deepctr、featexp
  特征值的编码方式（除了特定工具包所要求的特定格式）：
      frequency encoding：
      label encoding： 

  模型的训练方式：
      使用K折交叉验证来防止过拟合
      在把训练集划分为多个子集，在每个子集上单独训练一个子模型，最终将多个子模型的预测结果进行集成(求平均值，等）
      注意数据集的划分，不要破坏数据分布
  
  可以使用网格搜索GridSearchCV来在一定范围内自动调参
  余弦退火快照集成、快速几何集成：http://m.elecfans.com/article/672087.html
  多种模型融合的时候，要注意这两种模型尽量不要在同一维度，这样会得到较好的效果
  



